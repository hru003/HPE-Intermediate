{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <hr />\n",
    " Download the Dataset from the Github or from the link provided below.\n",
    " <hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/blastchar/telco-customer-churn/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <hr />\n",
    " Before starting with the notebook ensure pyspark is installed and working. To install and to find the spark use pip install as shown in the below cells.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "The following command adds the pyspark to sys.path at runtime. If the pyspark is not on the system path by default. It also prints the path of the spark.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HrushikeshaShastryBS\\miniconda3\\envs\\mlops1\\lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "print(findspark.find())\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Create a Spark Session\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Hyperparameter\") \\\n",
    "    .master('local[2]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Read the dataset into a dataframe.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn = spark.read.csv('customer_churn.csv', inferSchema=True, header=True, mode='DROPMALFORMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "# Data Fields Explained\n",
    "KEY FIELDS: <br>\n",
    "1. customerID: Customer ID <br>\n",
    "2. gender: Whether the customer is a male or a female<br>\n",
    "3. SeniorCitizen: Whether the customer is a senior citizen or not (1, 0)<br>\n",
    "4. Partner: Whether the customer has a partner or not (Yes, No)<br>\n",
    "5. Dependents: Whether the customer has dependents or not (Yes, No)<br>\n",
    "6. tenure: Number of months the customer has stayed with the company<br>\n",
    "7.mPhoneService: Whether the customer has a phone service or not (Yes, No)<br>\n",
    "8. MultipleLines: Whether the customer has multiple lines or not (Yes, No, No phone service)<br>\n",
    "9. InternetService: Customer’s internet service provider (DSL, Fiber optic, No)<br>\n",
    "10. OnlineSecurity: Whether the customer has online security or not (Yes, No, No internet service)<br>\n",
    "11. OnlineBackup: Whether the customer has online backup or not (Yes, No, No internet service)<br>\n",
    "12. DeviceProtection: Whether the customer has device protection or not (Yes, No, No internet service)<br>\n",
    "13. TechSupport: Whether the customer has tech support or not (Yes, No, No internet service)<br>\n",
    "14. StreamingTV: Whether the customer has streaming TV or not (Yes, No, No internet service)<br>\n",
    "15. StreamingMovies: Whether the customer has streaming movies or not (Yes, No, No internet service)<br>\n",
    "16. Contract: The contract term of the customer (Month-to-month, One year, Two year)<br>\n",
    "17. PaperlessBilling: Whether the customer has paperless billing or not (Yes, No)<br>\n",
    "18. PaymentMethod: The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))<br>\n",
    "19. MonthlyCharges: The amount charged to the customer monthly<br>\n",
    "20. TotalCharges: The total amount charged to the customer<br>\n",
    "\n",
    "PREDICTOR FIELD:<br>\n",
    "1. Churn: Whether the customer churned or not (Yes or No)<br>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " customerID       | 7590-VHVEG       \n",
      " gender           | Female           \n",
      " SeniorCitizen    | 0                \n",
      " Partner          | Yes              \n",
      " Dependents       | No               \n",
      " tenure           | 1                \n",
      " PhoneService     | No               \n",
      " MultipleLines    | No phone service \n",
      " InternetService  | DSL              \n",
      " OnlineSecurity   | No               \n",
      " OnlineBackup     | Yes              \n",
      " DeviceProtection | No               \n",
      " TechSupport      | No               \n",
      " StreamingTV      | No               \n",
      " StreamingMovies  | No               \n",
      " Contract         | Month-to-month   \n",
      " PaperlessBilling | Yes              \n",
      " PaymentMethod    | Electronic check \n",
      " MonthlyCharges   | 29.85            \n",
      " TotalCharges     | 29.85            \n",
      " Churn            | No               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_churn.show(1, truncate=False ,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Display the data type of the coulmns.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, customerID: string, gender: string, SeniorCitizen: string, Partner: string, Dependents: string, tenure: string, PhoneService: string, MultipleLines: string, InternetService: string, OnlineSecurity: string, OnlineBackup: string, DeviceProtection: string, TechSupport: string, StreamingTV: string, StreamingMovies: string, Contract: string, PaperlessBilling: string, PaymentMethod: string, MonthlyCharges: string, TotalCharges: string, Churn: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(customer_churn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Dropping rows with NaN values\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 7043\n",
      "rows after dropna 7043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"rows: {}\".format(customer_churn.count()))\n",
    "customer_churn = customer_churn.dropna()\n",
    "print(\"rows after dropna\",format(customer_churn.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Import the pyspark modules required for pre-processing the data.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, NaiveBayes\n",
    "from pyspark.ml import Pipeline,Model\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "stringIndexer_1= StringIndexer(inputCol=\"gender\", outputCol=\"gender_IX\")\n",
    "stringIndexer_2 = StringIndexer(inputCol=\"Partner\", outputCol=\"Partner_IX\")\n",
    "stringIndexer_3 = StringIndexer(inputCol=\"Dependents\", outputCol=\"Dependents_IX\")\n",
    "stringIndexer_4 = StringIndexer(inputCol=\"PhoneService\", outputCol=\"PhoneService_IX\")\n",
    "stringIndexer_5 = StringIndexer(inputCol=\"MultipleLines\", outputCol=\"MultipleLines_IX\")\n",
    "stringIndexer_6 = StringIndexer(inputCol=\"InternetService\", outputCol=\"InternetService_IX\")\n",
    "stringIndexer_7 = StringIndexer(inputCol=\"OnlineSecurity\", outputCol=\"OnlineSecurity_IX\")\n",
    "stringIndexer_8 = StringIndexer(inputCol=\"OnlineBackup\", outputCol=\"OnlineBackup_IX\")\n",
    "stringIndexer_9 = StringIndexer(inputCol=\"DeviceProtection\", outputCol=\"DeviceProtection_IX\")\n",
    "stringIndexer_10 = StringIndexer(inputCol=\"TechSupport\", outputCol=\"TechSupport_IX\")\n",
    "stringIndexer_11= StringIndexer(inputCol=\"StreamingTV\", outputCol=\"StreamingTV_IX\")\n",
    "stringIndexer_12= StringIndexer(inputCol=\"StreamingMovies\", outputCol=\"StreamingMovies_IX\")\n",
    "stringIndexer_13= StringIndexer(inputCol=\"Contract\", outputCol=\"Contract_IX\")\n",
    "stringIndexer_14= StringIndexer(inputCol=\"PaperlessBilling\", outputCol=\"PaperlessBilling_IX\")\n",
    "stringIndexer_15= StringIndexer(inputCol=\"PaymentMethod\", outputCol=\"PaymentMethod_IX\")\n",
    "\n",
    "vectorAssembler_features = VectorAssembler(inputCols=[\"gender_IX\", \"Partner_IX\",\"Dependents_IX\",\"PhoneService_IX\",\"MultipleLines_IX\",\"InternetService_IX\",\"OnlineSecurity_IX\",\"OnlineBackup_IX\",\"DeviceProtection_IX\",\"TechSupport_IX\",\"StreamingTV_IX\",\"StreamingMovies_IX\",\"Contract_IX\",\"PaperlessBilling_IX\",\"PaymentMethod_IX\",\"MonthlyCharges\",\"tenure\"],outputCol=\"features\")\n",
    "stringIndexer_label = StringIndexer(inputCol=\"Churn\", outputCol=\"label\").fit(customer_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_churn = customer_churn.withColumn(\"TotalCharges\",customer_churn.TotalCharges.cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Create the pre-processing pipeline\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[])\n",
    "\n",
    "basePipeline = [stringIndexer_1, stringIndexer_2,stringIndexer_3, stringIndexer_4,stringIndexer_5, stringIndexer_6,stringIndexer_7, stringIndexer_8,stringIndexer_9, stringIndexer_10,stringIndexer_11, stringIndexer_12,stringIndexer_13, stringIndexer_14,stringIndexer_15,stringIndexer_label, vectorAssembler_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Define the Models for trying during the hyperparameter tuning and the associated hyperparameter values\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",maxIter=10)\n",
    "pl_lr = basePipeline + [lr]\n",
    "pg_lr = ParamGridBuilder()\\\n",
    "          .baseOn({pipeline.stages: pl_lr})\\\n",
    "          .addGrid(lr.regParam,[0.01, .04])\\\n",
    "          .addGrid(lr.elasticNetParam,[0.1, 0.4])\\\n",
    "          .build()\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "pl_rf = basePipeline + [rf]\n",
    "pg_rf = ParamGridBuilder()\\\n",
    "      .baseOn({pipeline.stages: pl_rf})\\\n",
    "      .addGrid(rf.numTrees, [10, 25, 50])\\\n",
    "      .build()\n",
    "\n",
    "\n",
    "paramGrid = pg_lr + pg_rf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Split the data into training and testing sets.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records: 4226\n",
      "Number of testing records : 2817\n"
     ]
    }
   ],
   "source": [
    "splitted_data = customer_churn.randomSplit([0.6, 0.4], 24)   \n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]\n",
    "\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Build a Crossvalidator for hyperparameter tuning\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainValidationSplit' object has no attribute 'settrainRatio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HRUSHI~1\\AppData\\Local\\Temp/ipykernel_13984/2042768707.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuning\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrainValidationSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainValidationSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0msetEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0msetEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TrainValidationSplit' object has no attribute 'settrainRatio'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "cv = TrainValidationSplit(trainRatio=0.8)\\\n",
    "      .setEstimator(pipeline)\\\n",
    "      .setEvaluator(BinaryClassificationEvaluator())\\\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "cvmodel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Display the stages of the pipeline.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexerModel: uid=StringIndexer_39f5ac29e14e, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_4250067337c7, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_52e3ab3eb2b2, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_a0f40540f598, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_f83ab1d32717, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_2897574f4b89, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_308c34d84af2, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_bea23b1bc45f, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_4028b262a702, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_c5f3118b577d, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_ac0f446e1268, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_9f9c871f6eb6, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_de8a9390b838, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_3dc77b096990, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_9215261266ed, handleInvalid=error,\n",
       " StringIndexerModel: uid=StringIndexer_b9c31786c784, handleInvalid=error,\n",
       " VectorAssembler_72f7e4a30675,\n",
       " RandomForestClassificationModel: uid=RandomForestClassifier_c8c466898947, numTrees=50, numClasses=2, numFeatures=17]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Get the best model determined from the hyperparameter tuning\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cvmodel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Generate the predictions for the test data.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Determine the count of correct and incorrect predictions\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correct predictions are  2213\n",
      "Number of In-Correct predictions are  604\n"
     ]
    }
   ],
   "source": [
    "correct = predictions.where(\"(label = prediction)\").count()\n",
    "incorrect = predictions.where(\"(label != prediction)\").count()\n",
    "\n",
    "print(\"Number of Correct predictions are \", correct)\n",
    "print(\"Number of In-Correct predictions are \", incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Display the actual label and the prediction columns\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Determine the values of the confusion matrix.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive 310\n",
      "True Negative 1903\n",
      "False Positive 167\n",
      "False Negative 437\n"
     ]
    }
   ],
   "source": [
    "tp = predictions.filter(\"label == 1.0 AND prediction == 1.0\").count()\n",
    "tn = predictions.where(\"label == 0.0 AND prediction == 0.0\").count()\n",
    "fp = predictions.where(\"label == 0.0 AND prediction == 1.0\").count()\n",
    "fn = predictions.where(\"label == 1.0 AND prediction == 0.0\").count()\n",
    "\n",
    "print(\"True Positive\", tp)\n",
    "print(\"True Negative\", tn)\n",
    "print(\"False Positive\", fp)\n",
    "print(\"False Negative\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Determine the Precision and Recall Values.\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.4149933065595716\n",
      "precision 0.649895178197065\n"
     ]
    }
   ],
   "source": [
    "r = float(tp)/(tp + fn)\n",
    "print(\"recall\", r)\n",
    "\n",
    "p = float(tp) / (tp + fp)\n",
    "print(\"precision\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "Stop the Spark Session\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "918816dcb90eb2f409d280fd921ab35213362b65b31855a310672842d91a763f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('mlops1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
